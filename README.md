# ğŸ§  MNIST Neural Network from Scratch (NumPy-only)
This repository contains a fully from-scratch neural network implementation using only Python and NumPy, applied to the MNIST handwritten digits dataset.

âš ï¸ **Note:** This project uses an educational design with individual Neuron objects per layer. While this drastically reduces performance and training speed, it greatly increases intuitiveness, readability, and ease of fine-tuning for learning purposes.

## ğŸ“ Project Structure

```
MNIST_CUSTOM/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/               # Training images (MNIST format)
â”‚   â””â”€â”€ test/                # Test images (MNIST format)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ 40k_v1_model.pkl     # Pre-trained model (40k parameters)
â”‚
â”œâ”€â”€ network/                 # Core neural network logic (built from scratch)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ neuron.py            # Neuron class (with forward/backward passes)
â”‚   â”œâ”€â”€ layer.py             # Layer class (collection of neurons)
â”‚   â””â”€â”€ neural_network.py    # Complete network (built from layers)
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ config.py            # ğŸ”§ Adjustable training parameters (epochs, LR, decay...)
â”‚   â”œâ”€â”€ data_loader.py       # Handles loading and preprocessing of MNIST data
â”‚   â””â”€â”€ train.py             # Main training logic
â”‚
â”œâ”€â”€ testing/
â”‚   â””â”€â”€ test.py              # Script to evaluate a trained model
â”‚
â”œâ”€â”€ main.py                  # Run this to test the model on custom images
â””â”€â”€ README.md                # This file
```

## ğŸ§© Key Concepts
### Neurons as Objects 
Each Neuron is implemented as a Python class with individual weights, biases, activation (ReLU), gradients, and update logic.

### Layer
A Layer is a simple container of Neurons that performs forward/backward propagation in sequence.

### Neural Network
The NeuralNetwork class combines layers, manages forward and backward passes, and updates parameters.

### No Frameworks
No TensorFlow, PyTorch or high-level abstractions. This is bare-metal NumPy to showcase whatâ€™s under the hood of deep learning.

## ğŸš€ Getting Started
### 1. Clone the repository
```bash
git clone https://github.com/your-username/MNIST-NeuralNet-Scratch.git
cd MNIST-NeuralNet-Scratch
```
### 2. Install Requirements
Only numpy (and pickle for saving/loading models) are needed:

```bash
pip install pickle
pip install numpy
```
### 3. Train the Network
You can configure your training run in training/config.py (e.g., learning rate, epochs, decay):

```python
learning_rate = 0.01
epochs = 15
batch_size = 64
decay = 0.99
```
Then run:

```bash
python training/train.py
```
### 4. Test an Existing Model
A pre-trained model is included (models/40k_v1_model.pkl):

```bash
python testing/test.py
```
Or run the model on an image via:

```bash
python main.py
```
## ğŸ§  Example: Neuron Class
```python
z = np.dot(inputs, self.weights) + self.bias
if self.use_relu:
    return self.relu(z)
return z
```
Each neuron tracks its last input and gradient, and applies ReLU conditionally for backpropagation.
